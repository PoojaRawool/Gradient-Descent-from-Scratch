# Gradient-Descent-from-Scratch
We will implement the gradient descent algorithm from scratch. <br>
The purpose is to Understand steps associated with Gradient Descent Optimization, Understand the significance of learning rate, Understand the various stopping conditions associated with GD optimization.<br>
Gradient Descent from Scratch:<br>
Consider the cost function ğ¸=(ğ›½+2)^2âˆ’5<br>
Perform Gradient Descent Optimization from scratch using Python, to obtain the optimal value of the ğ›½ parameter. <br>
Try atleast 3 different cost functions<br>
ğ¸1=ğ›½^4âˆ’6ğ›½^2+4ğ›½+12<br>
ğ¸2=(ğ›½âˆ’2)^2+5<br>
ğ¸3=(5/ğ‘¥^2)<br>
