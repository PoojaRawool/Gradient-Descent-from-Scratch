# Gradient-Descent-from-Scratch
We will implement the gradient descent algorithm from scratch. 
The purpose is to Understand steps associated with Gradient Descent Optimization, Understand the significance of learning rate, Understand the various stopping conditions associated with GD optimization.
Gradient Descent from Scratch:
Consider the cost function 𝐸=(𝛽+2)^2−5
Perform Gradient Descent Optimization from scratch using Python, to obtain the optimal value of the 𝛽 parameter. 
Try atleast 3 different cost functions
𝐸1=𝛽^4−6𝛽^2+4𝛽+12
𝐸2=(𝛽−2)^2+5
𝐸3=(5/𝑥^2)
