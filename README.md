# Gradient-Descent-from-Scratch
We will implement the gradient descent algorithm from scratch. <br>
The purpose is to Understand steps associated with Gradient Descent Optimization, Understand the significance of learning rate, Understand the various stopping conditions associated with GD optimization.<br>
Gradient Descent from Scratch:<br>
Consider the cost function 𝐸=(𝛽+2)^2−5<br>
Perform Gradient Descent Optimization from scratch using Python, to obtain the optimal value of the 𝛽 parameter. <br>
Try atleast 3 different cost functions<br>
𝐸1=𝛽^4−6𝛽^2+4𝛽+12<br>
𝐸2=(𝛽−2)^2+5<br>
𝐸3=(5/𝑥^2)<br>
